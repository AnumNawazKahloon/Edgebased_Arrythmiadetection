{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in txhe \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-11T21:51:33.297746Z","iopub.execute_input":"2022-03-11T21:51:33.298174Z","iopub.status.idle":"2022-03-11T21:51:34.026383Z","shell.execute_reply.started":"2022-03-11T21:51:33.298112Z","shell.execute_reply":"2022-03-11T21:51:34.0252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport random\nimport pickle\nimport itertools\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, label_ranking_average_precision_score, label_ranking_loss, coverage_error \n\nfrom sklearn.utils import shuffle\n\nfrom scipy.signal import resample\n\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nimport pickle\nfrom sklearn.preprocessing import OneHotEncoder\n\n\n\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Conv1D, MaxPooling1D, Softmax, Add, Flatten, Activation# , Dropout\nfrom keras import backend as K\nfrom keras.optimizers import Adam\nfrom keras.callbacks import LearningRateScheduler, ModelCheckpoint\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nimport math\nimport random\nimport pickle\nimport itertools\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nnp.random.seed(42)\nimport tensorflow as tf\nimport tensorflow.keras as keras\n","metadata":{"execution":{"iopub.status.busy":"2022-03-11T21:51:34.028293Z","iopub.execute_input":"2022-03-11T21:51:34.028639Z","iopub.status.idle":"2022-03-11T21:51:41.745768Z","shell.execute_reply.started":"2022-03-11T21:51:34.028595Z","shell.execute_reply":"2022-03-11T21:51:41.744714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1.  # DATA ACQUISITION *","metadata":{}},{"cell_type":"code","source":"print(os.getcwd())","metadata":{"execution":{"iopub.status.busy":"2022-03-11T21:51:41.747532Z","iopub.execute_input":"2022-03-11T21:51:41.748054Z","iopub.status.idle":"2022-03-11T21:51:41.756024Z","shell.execute_reply.started":"2022-03-11T21:51:41.747994Z","shell.execute_reply":"2022-03-11T21:51:41.754771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mit_test_data = pd.read_csv(\"../input/heartbeat/mitbih_test.csv\", header=None)\nmit_train_data = pd.read_csv(\"../input/heartbeat/mitbih_train.csv\", header=None)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T21:51:41.758Z","iopub.execute_input":"2022-03-11T21:51:41.759016Z","iopub.status.idle":"2022-03-11T21:51:52.1279Z","shell.execute_reply.started":"2022-03-11T21:51:41.758934Z","shell.execute_reply":"2022-03-11T21:51:52.12677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PRODUCE BALANCED DATASET train_df , test_df *","metadata":{}},{"cell_type":"code","source":"# There is a huge difference in the balanced of the classes.\n# Better choose the resample technique more than the class weights for the algorithms.\nfrom sklearn.utils import resample\n\ndf_1=mit_train_data[mit_train_data[187]==1]\ndf_2=mit_train_data[mit_train_data[187]==2]\ndf_3=mit_train_data[mit_train_data[187]==3]\ndf_4=mit_train_data[mit_train_data[187]==4]\ndf_0=(mit_train_data[mit_train_data[187]==0]).sample(n=20000,random_state=42)\n\ndf_1_upsample=resample(df_1,replace=True,n_samples=20000,random_state=123)\ndf_2_upsample=resample(df_2,replace=True,n_samples=20000,random_state=124)\ndf_3_upsample=resample(df_3,replace=True,n_samples=20000,random_state=125)\ndf_4_upsample=resample(df_4,replace=True,n_samples=20000,random_state=126)\n\ntrain_df=pd.concat([df_0,df_1_upsample,df_2_upsample,df_3_upsample,df_4_upsample])\n\n\ndf_11=mit_test_data[mit_train_data[187]==1]\ndf_22=mit_test_data[mit_train_data[187]==2]\ndf_33=mit_test_data[mit_train_data[187]==3]\ndf_44=mit_test_data[mit_train_data[187]==4]\ndf_00=(mit_test_data[mit_train_data[187]==0]).sample(n=20000,random_state=42)\n\ndf_11_upsample=resample(df_1,replace=True,n_samples=20000,random_state=123)\ndf_22_upsample=resample(df_2,replace=True,n_samples=20000,random_state=124)\ndf_33_upsample=resample(df_3,replace=True,n_samples=20000,random_state=125)\ndf_44_upsample=resample(df_4,replace=True,n_samples=20000,random_state=126)\n\ntest_df=pd.concat([df_00,df_11_upsample,df_22_upsample,df_33_upsample,df_44_upsample])\n\n\nequilibre=train_df[187].value_counts()\nprint(equilibre)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T21:51:52.130223Z","iopub.execute_input":"2022-03-11T21:51:52.130555Z","iopub.status.idle":"2022-03-11T21:51:52.600675Z","shell.execute_reply.started":"2022-03-11T21:51:52.130511Z","shell.execute_reply":"2022-03-11T21:51:52.599497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"ALL Train data\")\nprint(\"Type\\tCount\")\nprint((mit_train_data[187]).value_counts())\nprint(\"-------------------------\")\nprint(\"ALL Test data\")\nprint(\"Type\\tCount\")\nprint((mit_test_data[187]).value_counts())\n\nprint(\"ALL Balanced Train data\")\nprint(\"Type\\tCount\")\nprint((train_df[187]).value_counts())\nprint(\"-------------------------\")\nprint(\"ALL Balanced Test data\")\nprint(\"Type\\tCount\")\nprint((train_df[187]).value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-03-11T21:51:52.602387Z","iopub.execute_input":"2022-03-11T21:51:52.602734Z","iopub.status.idle":"2022-03-11T21:51:52.628101Z","shell.execute_reply.started":"2022-03-11T21:51:52.602684Z","shell.execute_reply":"2022-03-11T21:51:52.626911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  MODEL RNN LSTM GRU","metadata":{}},{"cell_type":"markdown","source":"#  USE IF SAMPLES ARE IN A ROW","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(\"../input/heartbeat/mitbih_test.csv\", header=None)\ntest = test.iloc[0,0:len(test.T)-1] # Remove last line cause it might be a Nan\ntest = pd.DataFrame(test)\n# NORMALIZING TEST DATA AMPLITUDE\nfrom sklearn.preprocessing import MinMaxScaler\n# load the dataset and print the first 5 rows\n# prepare data for normalization\nvalues = test.values\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler = scaler.fit(values)\nnormalized = scaler.transform(values)\nnormalized = pd.DataFrame(normalized)\nnormalized","metadata":{"execution":{"iopub.status.busy":"2022-03-11T21:51:52.629828Z","iopub.execute_input":"2022-03-11T21:51:52.630273Z","iopub.status.idle":"2022-03-11T21:51:53.929735Z","shell.execute_reply.started":"2022-03-11T21:51:52.630217Z","shell.execute_reply":"2022-03-11T21:51:53.92872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL LSTM RNN","metadata":{}},{"cell_type":"code","source":"from keras.datasets import imdb\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Bidirectional\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\n\nnp.random.seed(7)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T21:51:53.930866Z","iopub.execute_input":"2022-03-11T21:51:53.93117Z","iopub.status.idle":"2022-03-11T21:51:53.937671Z","shell.execute_reply.started":"2022-03-11T21:51:53.931135Z","shell.execute_reply":"2022-03-11T21:51:53.936721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nfrom keras.utils import to_categorical\n\nprint(\"--- X ---\")\n# X = mit_train_data.loc[:, mit_train_data.columns != 187]\nX = train_df.loc[:, mit_train_data.columns != 187]\nprint(X.head())\nprint(X.info())\n\nprint(\"--- Y ---\")\n# y = mit_train_data.loc[:, mit_train_data.columns == 187]\ny = train_df.loc[:, mit_train_data.columns == 187]\ny = to_categorical(y)\n\nprint(\"--- testX ---\")\n#testX = mit_test_data.loc[:, mit_test_data.columns != 187]\ntestX = test_df.loc[:, mit_test_data.columns != 187]\nprint(testX.head())\nprint(testX.info())\n\nprint(\"--- testy ---\")\n#testy = mit_test_data.loc[:, mit_test_data.columns == 187]\ntesty = test_df.loc[:, mit_test_data.columns == 187]\ntesty = to_categorical(testy)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T21:51:53.938979Z","iopub.execute_input":"2022-03-11T21:51:53.939291Z","iopub.status.idle":"2022-03-11T21:51:54.161084Z","shell.execute_reply.started":"2022-03-11T21:51:53.93925Z","shell.execute_reply":"2022-03-11T21:51:54.160004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create the model.\nfrom keras.callbacks import History \nhistory = History()\nembedding_vecor_length = 187\n\nmodel = Sequential()\n\nmodel.add(Dense(50, activation='relu', input_shape=(187,)))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(5, activation='softmax'))\n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistory=model.fit(X, y, epochs=10)\n\nprint(\"Evaluation: \")\nmse, acc = model.evaluate(testX, testy)\nprint('mean_squared_error :', mse)\nprint('accuracy:', acc)\n\n\n\n#model = Sequential()\n#model = Bidirectional(model)\n\n#model.add(Embedding(1000, embedding_vecor_length, input_length=187))\n#model.add(LSTM(187))\n#model.add(Dense(5, activation='softmax'))\n#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n#print(model.summary())\n#history = model.fit(X, y, validation_data=(testX, testy), epochs=3, batch_size=8)\n\n\n#Dropout is a powerful technique for combating overfitting in your LSTM models \n#model = Sequential()\n#model.add(Embedding(1000, embedding_vecor_length, input_length=187))\n#model.add(LSTM(50, dropout=0.001, recurrent_dropout=0.001))\n#model.add(Dense(5, activation='softmax'))\n#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n#print(model.summary())\n#history = model.fit(X, y, validation_data=(testX, testy), epochs=50, batch_size=128)\n\n\n\n## SAVE MODEL ##\n# serialize model to JSON\n#model_json = model.to_json()\n#with open(\"1model.json\", \"w\") as json_file:\n #   json_file.write(model_json)\n# serialize weights to HDF5\n#model.save_weights(\"1model.h5\")\n#print(\"Saved model to disk\")","metadata":{"execution":{"iopub.status.busy":"2022-03-11T21:51:54.162359Z","iopub.execute_input":"2022-03-11T21:51:54.162762Z","iopub.status.idle":"2022-03-11T21:52:42.981221Z","shell.execute_reply.started":"2022-03-11T21:51:54.16271Z","shell.execute_reply":"2022-03-11T21:52:42.980211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate Model","metadata":{}},{"cell_type":"raw","source":"mse, acc = model.evaluate(testX, testy)\nprint('mean_squared_error :', mse)\nprint('accuracy:', acc)","metadata":{}},{"cell_type":"code","source":"# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\n\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-11T21:52:42.982417Z","iopub.execute_input":"2022-03-11T21:52:42.982727Z","iopub.status.idle":"2022-03-11T21:52:43.425749Z","shell.execute_reply.started":"2022-03-11T21:52:42.982691Z","shell.execute_reply":"2022-03-11T21:52:43.424696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Î‘ccuracy and prediction scores","metadata":{}},{"cell_type":"code","source":"y_pred = model.predict(testX, batch_size=1000)\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, label_ranking_average_precision_score, label_ranking_loss, coverage_error \n\nprint(classification_report(testy.argmax(axis=1), y_pred.argmax(axis=1)))","metadata":{"execution":{"iopub.status.busy":"2022-03-11T21:52:43.427392Z","iopub.execute_input":"2022-03-11T21:52:43.42807Z","iopub.status.idle":"2022-03-11T21:52:43.831517Z","shell.execute_reply.started":"2022-03-11T21:52:43.428006Z","shell.execute_reply":"2022-03-11T21:52:43.830432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}